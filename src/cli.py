#!/usr/bin/env python
"""
MCP RAG Server CLI

ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã®ã‚¯ãƒªã‚¢ã¨ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã‚’è¡Œã†ãŸã‚ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from dotenv import load_dotenv

from .rag_tools import create_rag_service_from_env


def setup_logging():
    """
    ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®E    """
    # ãƒ­ã‚°ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæE
    os.makedirs("logs", exist_ok=True)

    # ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®E    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(os.path.join("logs", "mcp_rag_cli.log"), encoding="utf-8"),
        ],
    )
    return logging.getLogger("cli")


def clear_index():
    """
    ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
    """
    logger = setup_logging()
    logger.info("ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãE¾ãE..")

    # ç’°å¢E¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # RAGã‚µãƒ¼ãƒ“ã‚¹ã®ä½œæE
    rag_service = create_rag_service_from_env()

    # å‡¦çE¸ˆã¿ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    processed_dir = os.environ.get("PROCESSED_DIR", "data/processed")

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®å‰Šé™¤
    registry_path = Path(processed_dir) / "file_registry.json"
    if registry_path.exists():
        try:
            registry_path.unlink()
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {registry_path}")
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {registry_path}")
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    # ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢
    result = rag_service.clear_index()

    if result["success"]:
        logger.info(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸEEresult['deleted_count']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤EE)
        print(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸEEresult['deleted_count']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤EE)
    else:
        logger.error(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'ä¸æEãªã‚¨ãƒ©ãƒ¼')}")
        print(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'ä¸æEãªã‚¨ãƒ©ãƒ¼')}")
        sys.exit(1)


def index_documents(directory_path, chunk_size=500, chunk_overlap=100, incremental=False):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚E
    Args:
        directory_path: ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã‚‹ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºEˆæ–‡å­—æ•°EE        chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ãEã‚ªãƒ¼ãƒãEãƒ©ãƒEEEˆæ–‡å­—æ•°EE        incremental: å·®åˆEEã¿ã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ã‹ã©ãE‹
    """
    logger = setup_logging()
    if incremental:
        logger.info(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' å†EEå·®åˆEƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã—ã¦ãE¾ãE..")
    else:
        logger.info(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã—ã¦ãE¾ãE..")

    # ç’°å¢E¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèªE    if not os.path.exists(directory_path):
        logger.error(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    if not os.path.isdir(directory_path):
        logger.error(f"'{directory_path}' ã¯ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        print(f"ã‚¨ãƒ©ãƒ¼: '{directory_path}' ã¯ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    # RAGã‚µãƒ¼ãƒ“ã‚¹ã®ä½œæE
    rag_service = create_rag_service_from_env()

    # å‡¦çE¸ˆã¿ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    processed_dir = os.environ.get("PROCESSED_DIR", "data/processed")

    # ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã‚’å®Ÿè¡E    if incremental:
        print(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' å†EEå·®åˆEƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã—ã¦ãE¾ãE..")
    else:
        print(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{directory_path}' å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã—ã¦ãE¾ãE..")

    # é€²æ—çŠ¶æ³ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ã‚«ã‚¦ãƒ³ã‚¿
    processed_files = 0

    # å‡¦çE‰ã«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾E    total_files = 0
    for root, _, files in os.walk(directory_path):
        for file in files:
            file_path = os.path.join(root, file)
            ext = os.path.splitext(file_path)[1].lower()
            if ext in [".md", ".markdown", ".txt", ".pdf", ".ppt", ".pptx", ".doc", ".docx"]:
                total_files += 1

    print(f"åˆè¨E{total_files} å€‹ãEãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã¾ã—ãŸ...")

    # å…EERAGServiceã®index_documentsãƒ¡ã‚½ãƒEƒ‰ã‚’å‘¼ã³å‡ºã™å‰ã«ã€E    # DocumentProcessorã®process_directoryãƒ¡ã‚½ãƒEƒ‰ã‚’ã‚ªãƒ¼ãƒãEãƒ©ã‚¤ãƒ‰ã—ã¦é€²æ—ã‚’è¡¨ç¤º
    original_process_directory = rag_service.document_processor.process_directory

    def process_directory_with_progress(source_dir, processed_dir, chunk_size=500, overlap=100, incremental=False):
        nonlocal processed_files
        results = []
        source_directory = Path(source_dir)

        if not source_directory.exists() or not source_directory.is_dir():
            logger.error(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{source_dir}' ãŒè¦‹ã¤ã‹ã‚‰ãªãE‹ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            raise FileNotFoundError(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{source_dir}' ãŒè¦‹ã¤ã‹ã‚‰ãªãE‹ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“")

        # ã‚µãƒãEãƒˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å…¨ã¦å–å¾E        all_extensions = []
        for ext_list in rag_service.document_processor.SUPPORTED_EXTENSIONS.values():
            all_extensions.extend(ext_list)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        files = []
        for ext in all_extensions:
            files.extend(list(source_directory.glob(f"**/*{ext}")))

        logger.info(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{source_dir}' å†E« {len(files)} å€‹ãEãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        # å·®åˆEEçEEå ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’èª­ã¿è¾¼ã‚€
        if incremental:
            file_registry = rag_service.document_processor.load_file_registry(processed_dir)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ {len(file_registry)} å€‹ãEãƒ•ã‚¡ã‚¤ãƒ«æƒE ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

            # å‡¦çE¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®E            files_to_process = []
            for file_path in files:
                str_path = str(file_path)
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒEEã‚¿ã‚’å–å¾E                current_metadata = rag_service.document_processor.get_file_metadata(str_path)

                # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«å­˜åœ¨ã—ãªãE€ã¾ãŸãEãƒãƒƒã‚·ãƒ¥å€¤ãŒå¤‰æ›´ã•ã‚Œã¦ãE‚‹å ´åˆãEã¿å‡¦çE                if (
                    str_path not in file_registry
                    or file_registry[str_path]["hash"] != current_metadata["hash"]
                    or file_registry[str_path]["mtime"] != current_metadata["mtime"]
                    or file_registry[str_path]["size"] != current_metadata["size"]
                ):
                    files_to_process.append(file_path)
                    # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’æ›´æ–°
                    file_registry[str_path] = current_metadata

            print(f"å‡¦çE¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_to_process)} / {len(files)}")

            # åEƒ•ã‚¡ã‚¤ãƒ«ã‚’åEçE            for i, file_path in enumerate(files_to_process):
                try:
                    file_results = rag_service.document_processor.process_file(
                        str(file_path), processed_dir, chunk_size, overlap
                    )
                    results.extend(file_results)
                    processed_files += 1
                    print(
                        f"å‡¦çE¸­... {processed_files}/{len(files_to_process)} ãƒ•ã‚¡ã‚¤ãƒ« ({(processed_files / len(files_to_process) * 100):.1f}%): {file_path}"
                    )
                except Exception as e:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ã®å‡¦çE¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚åEçE‚’ç¶šè¡E                    continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’ä¿å­E            rag_service.document_processor.save_file_registry(processed_dir, file_registry)
        else:
            # å·®åˆEEçE§ãªãE ´åˆãEå…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åEçE            for i, file_path in enumerate(files):
                try:
                    file_results = rag_service.document_processor.process_file(
                        str(file_path), processed_dir, chunk_size, overlap
                    )
                    results.extend(file_results)
                    processed_files += 1
                    print(
                        f"å‡¦çE¸­... {processed_files}/{total_files} ãƒ•ã‚¡ã‚¤ãƒ« ({(processed_files / total_files * 100):.1f}%): {file_path}"
                    )
                except Exception as e:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ã®å‡¦çE¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚åEçE‚’ç¶šè¡E                    continue

            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦çEEå ´åˆã‚‚ã€æ–°ã—ã„ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’ä½œæEã—ã¦ä¿å­E            file_registry = {}
            for file_path in files:
                str_path = str(file_path)
                file_registry[str_path] = rag_service.document_processor.get_file_metadata(str_path)
            rag_service.document_processor.save_file_registry(processed_dir, file_registry)

        logger.info(f"ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒª '{source_dir}' å†EEãƒ•ã‚¡ã‚¤ãƒ«ã‚’åEçE—ã¾ã—ãŸEˆåˆè¨E{len(results)} ãƒãƒ£ãƒ³ã‚¯EE)
        return results

    # é€²æ—è¡¨ç¤ºä»˜ãã®å‡¦çEƒ¡ã‚½ãƒEƒ‰ã«ç½®ãæ›ãE    rag_service.document_processor.process_directory = process_directory_with_progress

    # ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã‚’å®Ÿè¡E    result = rag_service.index_documents(directory_path, processed_dir, chunk_size, chunk_overlap, incremental)

    # å…EEãƒ¡ã‚½ãƒEƒ‰ã«æˆ»ãE    rag_service.document_processor.process_directory = original_process_directory

    if result["success"]:
        incremental_text = "å·®åˆE if incremental else "å…¨ã¦"
        logger.info(
            f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ãŒå®ŒäºE—ã¾ã—ãŸEEincremental_text}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åEçE€{result['document_count']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€{result['processing_time']:.2f} ç§’ï¼E
        )
        print(
            f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ãŒå®ŒäºE—ã¾ã—ãŸEEincremental_text}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åEçE¼‰\n"
            f"- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {result['document_count']}\n"
            f"- å‡¦çE™‚é–E {result['processing_time']:.2f} ç§’\n"
            f"- ãƒ¡ãƒE‚»ãƒ¼ã‚¸: {result.get('message', '')}"
        )
    else:
        logger.error(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'ä¸æEãªã‚¨ãƒ©ãƒ¼')}")
        print(
            f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ\n"
            f"- ã‚¨ãƒ©ãƒ¼: {result.get('error', 'ä¸æEãªã‚¨ãƒ©ãƒ¼')}\n"
            f"- å‡¦çE™‚é–E {result['processing_time']:.2f} ç§E
        )
        sys.exit(1)


def get_document_count():
    """
    ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã™ã‚E    """
    logger = setup_logging()
    logger.info("ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã¦ãE¾ãE..")

    # ç’°å¢E¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()

    # RAGã‚µãƒ¼ãƒ“ã‚¹ã®ä½œæE
    rag_service = create_rag_service_from_env()

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾E    try:
        count = rag_service.get_document_count()
        logger.info(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {count}")
        print(f"ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {count}")
    except Exception as e:
        logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        sys.exit(1)


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°

    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã€E©åˆEªå‡¦çE‚’å®Ÿè¡Œã—ã¾ã™ã€E    """
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æE    parser = argparse.ArgumentParser(
        description="MCP RAG Server CLI - ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã®ã‚¯ãƒªã‚¢ã¨ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã‚’è¡Œã†ãŸã‚ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"
    )
    subparsers = parser.add_subparsers(dest="command", help="å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒE)

    # clearã‚³ãƒãƒ³ãƒE    subparsers.add_parser("clear", help="ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹")

    # indexã‚³ãƒãƒ³ãƒE    index_parser = subparsers.add_parser("index", help="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚E)
    index_parser.add_argument(
        "--directory",
        "-d",
        default=os.environ.get("SOURCE_DIR", "./data/source"),
        help="ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã‚‹ãƒE‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹",
    )
    index_parser.add_argument("--chunk-size", "-s", type=int, default=500, help="ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºEˆæ–‡å­—æ•°EE)
    index_parser.add_argument("--chunk-overlap", "-o", type=int, default=100, help="ãƒãƒ£ãƒ³ã‚¯é–“ãEã‚ªãƒ¼ãƒãEãƒ©ãƒEEEˆæ–‡å­—æ•°EE)
    index_parser.add_argument("--incremental", "-i", action="store_true", help="å·®åˆEEã¿ã‚’ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹åŒ–ã™ã‚E)

    # countã‚³ãƒãƒ³ãƒE    subparsers.add_parser("count", help="ã‚¤ãƒ³ãƒEƒƒã‚¯ã‚¹å†EEãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã™ã‚E)

    args = parser.parse_args()

    # ã‚³ãƒãƒ³ãƒ‰ã«å¿œã˜ãŸåEçE‚’å®Ÿè¡E    if args.command == "clear":
        clear_index()
    elif args.command == "index":
        index_documents(args.directory, args.chunk_size, args.chunk_overlap, args.incremental)
    elif args.command == "count":
        get_document_count()
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
